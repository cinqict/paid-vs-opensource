{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2ffdfe-8265-4767-8f89-55e81926e8a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## MLflow Train, Cross Validation, Hyperparameter Tuning and deploy to databricks as a REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a85d8828-5dee-4b4d-9207-77c26a35b1f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting hyperopt\n  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from hyperopt) (1.21.5)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from hyperopt) (1.7.3)\nCollecting cloudpickle\n  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\nCollecting networkx>=2.2\n  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\nCollecting future\n  Using cached future-0.18.3-py3-none-any.whl\nCollecting py4j\n  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\nCollecting tqdm\n  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\nRequirement already satisfied: six in /databricks/python3/lib/python3.9/site-packages (from hyperopt) (1.16.0)\nInstalling collected packages: tqdm, py4j, networkx, future, cloudpickle, hyperopt\nSuccessfully installed cloudpickle-2.2.1 future-0.18.3 hyperopt-0.2.7 networkx-3.1 py4j-0.10.9.7 tqdm-4.65.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting mlflow\n  Using cached mlflow-2.4.1-py3-none-any.whl (18.1 MB)\nCollecting gunicorn<21\n  Using cached gunicorn-20.1.0-py3-none-any.whl (79 kB)\nCollecting docker<7,>=4.0.0\n  Using cached docker-6.1.3-py3-none-any.whl (148 kB)\nRequirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2021.3)\nCollecting pyyaml<7,>=5.1\n  Using cached PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\nCollecting gitpython<4,>=2.1.0\n  Using cached GitPython-3.1.31-py3-none-any.whl (184 kB)\nRequirement already satisfied: cloudpickle<3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-26a46ea7-f5b0-4f94-b947-a974c7949848/lib/python3.9/site-packages (from mlflow) (2.2.1)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.19.4)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.27.1)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\nCollecting alembic!=1.10.0,<2\n  Using cached alembic-1.11.1-py3-none-any.whl (224 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\nCollecting Flask<3\n  Using cached Flask-2.3.2-py3-none-any.whl (96 kB)\nRequirement already satisfied: pyarrow<13,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.4)\nCollecting markdown<4,>=3.3\n  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\nCollecting querystring-parser<2\n  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nCollecting importlib-metadata!=4.7.0,<7,>=3.7.0\n  Using cached importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\nCollecting sqlalchemy<3,>=1.4.0\n  Using cached SQLAlchemy-2.0.16-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\nRequirement already satisfied: packaging<24 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (21.3)\nCollecting sqlparse<1,>=0.4.0\n  Using cached sqlparse-0.4.4-py3-none-any.whl (41 kB)\nCollecting databricks-cli<1,>=0.8.7\n  Using cached databricks_cli-0.17.7-py3-none-any.whl\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (8.0.4)\nCollecting Mako\n  Using cached Mako-1.2.4-py3-none-any.whl (78 kB)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.1.1)\nRequirement already satisfied: urllib3<2.0.0,>=1.26.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.9)\nCollecting tabulate>=0.7.7\n  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\nCollecting oauthlib>=3.1.0\n  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\nCollecting pyjwt>=1.7.0\n  Using cached PyJWT-2.7.0-py3-none-any.whl (22 kB)\nCollecting websocket-client>=0.32.0\n  Using cached websocket_client-1.5.3-py3-none-any.whl (56 kB)\nCollecting Werkzeug>=2.3.3\n  Using cached Werkzeug-2.3.6-py3-none-any.whl (242 kB)\nCollecting click<9,>=7.0\n  Using cached click-8.1.3-py3-none-any.whl (96 kB)\nCollecting itsdangerous>=2.1.2\n  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\nCollecting blinker>=1.6.2\n  Using cached blinker-1.6.2-py3-none-any.whl (13 kB)\nCollecting Jinja2<4,>=2.11\n  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\nCollecting gitdb<5,>=4.0.1\n  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\nCollecting smmap<6,>=3.0.1\n  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.9/site-packages (from gunicorn<21->mlflow) (61.2.0)\nCollecting zipp>=0.5\n  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2021.10.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\nCollecting typing-extensions>=4\n  Using cached typing_extensions-4.6.3-py3-none-any.whl (31 kB)\nCollecting greenlet!=0.4.17\n  Using cached greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\nCollecting MarkupSafe>=2.0\n  Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nInstalling collected packages: zipp, typing-extensions, smmap, MarkupSafe, greenlet, Werkzeug, websocket-client, tabulate, sqlalchemy, pyjwt, oauthlib, Mako, Jinja2, itsdangerous, importlib-metadata, gitdb, click, blinker, sqlparse, querystring-parser, pyyaml, markdown, gunicorn, gitpython, Flask, docker, databricks-cli, alembic, mlflow\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-26a46ea7-f5b0-4f94-b947-a974c7949848\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.0.1\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-26a46ea7-f5b0-4f94-b947-a974c7949848\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n  Attempting uninstall: Jinja2\n    Found existing installation: Jinja2 2.11.3\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-26a46ea7-f5b0-4f94-b947-a974c7949848\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-26a46ea7-f5b0-4f94-b947-a974c7949848\n    Can't uninstall 'click'. No files were found to uninstall.\nSuccessfully installed Flask-2.3.2 Jinja2-3.1.2 Mako-1.2.4 MarkupSafe-2.1.3 Werkzeug-2.3.6 alembic-1.11.1 blinker-1.6.2 click-8.1.3 databricks-cli-0.17.7 docker-6.1.3 gitdb-4.0.10 gitpython-3.1.31 greenlet-2.0.2 gunicorn-20.1.0 importlib-metadata-6.6.0 itsdangerous-2.1.2 markdown-3.4.3 mlflow-2.4.1 oauthlib-3.2.2 pyjwt-2.7.0 pyyaml-6.0 querystring-parser-1.2.4 smmap-5.0.0 sqlalchemy-2.0.16 sqlparse-0.4.4 tabulate-0.9.0 typing-extensions-4.6.3 websocket-client-1.5.3 zipp-3.15.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting xgboost\n  Using cached xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl (200.3 MB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.21.5)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.7.3)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-1.7.5\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install hyperopt\n",
    "%pip install mlflow\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b8ceaa-3d1e-4596-9814-e021110e88c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from math import exp\n",
    "import mlflow.xgboost\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "from mlflow.tracking import MlflowClient\n",
    "import cloudpickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727e4f50-4f95-4c44-a5e7-32344823a8f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_historical_weather(\n",
    "    lat=52.377956,\n",
    "    lon=4.897070,\n",
    "    start_date=\"2022-01-01\",\n",
    "    end_date=\"2022-12-31\",\n",
    "    feature_list=[\n",
    "        \"temperature_2m\",\n",
    "        \"relativehumidity_2m\",\n",
    "        \"windspeed_10m\",\n",
    "        \"rain\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    Get historical weather data from open-meteo.com\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat: float\n",
    "        Latitude of the location you want to get the weather for.\n",
    "    lon: float\n",
    "        Longitude of the location you want to get the weather for.\n",
    "    start_date: str\n",
    "        Start date of the period you want to get the weather for.\n",
    "    end_date: str\n",
    "        End date of the period you want to get the weather for.\n",
    "    feature_list: list\n",
    "        List of features you want to get the weather for.\n",
    "        Options: \"temperature_2m\", \"relativehumidity_2m\",\n",
    "        \"windspeed_10m\", \"rain\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: pd.DataFrame\n",
    "        Data frame containing the weather data.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from utils import get_historical_weather\n",
    "    >>> df = get_historical_weather()\n",
    "    >>> df.head()\n",
    "\n",
    "    \"\"\"\n",
    "    url = f\"https://archive-api.open-meteo.com/v1/era5?latitude={lat}&longitude={lon}&start_date={start_date}&end_date={end_date}&hourly={','.join(feature_list)}\"\n",
    "    response = requests.get(url)\n",
    "    return pd.DataFrame(response.json()[\"hourly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16bb7a4-3375-469f-9a16-1bc0dcbc76b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = (\n",
    "    spark.read.load(\"dbfs:/user/hive/warehouse/disruptions_2011_2021\")\n",
    "    .withColumn(\"start_time\", col(\"start_time\").cast(\"string\"))\n",
    "    .withColumn(\"end_time\", col(\"end_time\").cast(\"string\"))\n",
    "    .toPandas()\n",
    "    .assign(\n",
    "        **{\n",
    "            \"start_time\": lambda x: pd.to_datetime(x[\"start_time\"]),\n",
    "            # \"end_time\": lambda x: pd.to_datetime(x[\"end_time\"]),\n",
    "            \"date\": lambda x: pd.to_datetime(x[\"start_time\"]).dt.date,\n",
    "        }\n",
    "    )\n",
    "    .groupby(\"date\")\n",
    "    .agg({\"duration_minutes\": \"sum\"})\n",
    "    # .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6ba73f-4220-47be-9b81-67980963cc53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_df = (\n",
    "    get_historical_weather(\n",
    "        lat=52.520008,\n",
    "        lon=13.404954,\n",
    "        start_date=str(train_df.index.min()),\n",
    "        end_date=str(train_df.index.max()),\n",
    "    )\n",
    "    .assign(**{\"date\": lambda x: pd.to_datetime(x[\"time\"]).dt.date})\n",
    "    .groupby(\"date\")\n",
    "    .agg({\"temperature_2m\": [\"mean\", \"min\", \"max\"], \"rain\": \"sum\"})\n",
    ")\n",
    "weather_df.columns = [\"_\".join(col) for col in weather_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d5517b-0329-4dad-8805-e682f7cd5f03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (pd.merge(train_df, weather_df, on=\"date\", how=\"left\")\n",
    "      .loc[lambda x: x[\"duration_minutes\"] < 20000] # remove outliers\n",
    "      .dropna()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1818bd1-b5a2-4ebb-aa82-b0a4bd0c4458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target = \"duration_minutes\"\n",
    "X = df.drop([target], axis=1)\n",
    "y = df[target]\n",
    " \n",
    "# Split out the training data\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.6, random_state=123)\n",
    " \n",
    "# Split the remaining data equally into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51bd86c-8f74-4f43-a173-23c0bb984464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_space = { \n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt','goss']),\n",
    "    'metric': hp.choice('metric',['rmse']),\n",
    "    'max_depth':scope.int(hp.quniform('max_depth', 2, 16, 1)),\n",
    "    'min_data_in_leaf': scope.int(hp.quniform('min_data_in_leaf', 30, 150, 1)),\n",
    "    'num_leaves': scope.int(hp.quniform('num_leaves', 30, 150, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 10),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 10),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "    \"feature_pre_filter\": hp.choice(\"feature_pre_filter\",[False]),\n",
    "    'seed': 123, # Set a seed for deterministic training\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38c2a33-59fc-4b03-842b-d164451859b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model(params):\n",
    "    # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.\n",
    "    mlflow.xgboost.autolog()\n",
    "    with mlflow.start_run(nested=True):\n",
    "        train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "        validation = xgb.DMatrix(data=X_val, label=y_val)\n",
    "        # Pass in the validation set so xgb can track an evaluation metric. XGBoost terminates training when the evaluation metric\n",
    "        # is no longer improving.\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=train,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(validation, \"validation\")],\n",
    "            early_stopping_rounds=50,\n",
    "        )\n",
    "        validation_predictions = booster.predict(validation)\n",
    "        metric_score = np.sqrt(mean_squared_error(y_val, validation_predictions))\n",
    "        mlflow.log_metric(\"RMSE\", metric_score)\n",
    "\n",
    "        signature = infer_signature(X_train, booster.predict(train))\n",
    "        mlflow.xgboost.log_model(booster, \"model\", signature=signature)\n",
    "\n",
    "        # Set the loss to -1*auc_score so fmin maximizes the auc_score\n",
    "        return {\n",
    "            \"status\": STATUS_OK,\n",
    "            \"loss\": -1 * metric_score,\n",
    "            \"booster\": booster.attributes(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ffc7143-98b4-4ad5-8d18-347838f4e376",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Greater parallelism will lead to speedups, but a less optimal hyperparameter sweep. \n",
    "# A reasonable value for parallelism is the square root of max_evals.\n",
    "spark_trials = SparkTrials(parallelism=10)\n",
    "# mlflow.set_experiment(\"/Users/james.twose@cinqict.nl/disruptions_xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7207cf4-1de6-4bfd-9195-83af14eaeb51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/96 [00:00<?, ?trial/s, best loss=?]\r  1%|          | 1/96 [00:14<22:15, 14.06s/trial, best loss: -1712.1590894160886]\r  3%|▎         | 3/96 [00:15<06:13,  4.02s/trial, best loss: -1789.2112967147025]\r  4%|▍         | 4/96 [00:16<04:33,  2.98s/trial, best loss: -1789.2112967147025]\r  6%|▋         | 6/96 [00:17<02:37,  1.75s/trial, best loss: -1800.2259562156223]\r  7%|▋         | 7/96 [00:19<02:41,  1.82s/trial, best loss: -1800.2259562156223]\r  8%|▊         | 8/96 [00:20<02:20,  1.60s/trial, best loss: -1800.2259562156223]\r 10%|█         | 10/96 [00:25<02:51,  2.00s/trial, best loss: -1800.2259562156223]\r 11%|█▏        | 11/96 [00:26<02:29,  1.76s/trial, best loss: -1800.2259562156223]\r 12%|█▎        | 12/96 [00:27<02:12,  1.57s/trial, best loss: -1800.2259562156223]\r 15%|█▍        | 14/96 [00:28<01:32,  1.13s/trial, best loss: -1800.2259562156223]\r 16%|█▌        | 15/96 [00:30<01:48,  1.34s/trial, best loss: -1800.2259562156223]\r 17%|█▋        | 16/96 [00:31<01:40,  1.26s/trial, best loss: -1800.2259562156223]\r 19%|█▉        | 18/96 [00:36<02:19,  1.79s/trial, best loss: -1800.2259562156223]\r 20%|█▉        | 19/96 [00:37<02:04,  1.61s/trial, best loss: -1800.2259562156223]\r 21%|██        | 20/96 [00:38<01:51,  1.47s/trial, best loss: -1800.2259562156223]\r 23%|██▎       | 22/96 [00:39<01:19,  1.07s/trial, best loss: -1896.7862253359554]\r 24%|██▍       | 23/96 [00:41<01:34,  1.30s/trial, best loss: -1896.7862253359554]\r 25%|██▌       | 24/96 [00:42<01:28,  1.23s/trial, best loss: -1896.7862253359554]\r 28%|██▊       | 27/96 [00:47<01:41,  1.46s/trial, best loss: -1896.7862253359554]\r 29%|██▉       | 28/96 [00:48<01:33,  1.38s/trial, best loss: -1896.7862253359554]\r 31%|███▏      | 30/96 [00:50<01:22,  1.26s/trial, best loss: -1896.7862253359554]\r 32%|███▏      | 31/96 [00:52<01:32,  1.42s/trial, best loss: -1912.1498544145943]\r 33%|███▎      | 32/96 [00:53<01:25,  1.33s/trial, best loss: -1912.1498544145943]\r 36%|███▋      | 35/96 [00:58<01:31,  1.50s/trial, best loss: -1912.1498544145943]\r 38%|███▊      | 36/96 [00:59<01:24,  1.41s/trial, best loss: -1912.1498544145943]\r 40%|███▉      | 38/96 [01:01<01:14,  1.28s/trial, best loss: -1912.1498544145943]\r 41%|████      | 39/96 [01:03<01:21,  1.43s/trial, best loss: -1912.1498544145943]\r 42%|████▏     | 40/96 [01:04<01:15,  1.34s/trial, best loss: -1912.1498544145943]\r 43%|████▎     | 41/96 [01:09<02:03,  2.24s/trial, best loss: -1912.1498544145943]\r 45%|████▍     | 43/96 [01:10<01:21,  1.53s/trial, best loss: -1912.1498544145943]\r 46%|████▌     | 44/96 [01:12<01:13,  1.42s/trial, best loss: -1912.1498544145943]\r 49%|████▉     | 47/96 [01:17<01:16,  1.56s/trial, best loss: -1912.1498544145943]\r 51%|█████     | 49/96 [01:19<01:05,  1.39s/trial, best loss: -1912.1498544145943]\r 53%|█████▎    | 51/96 [01:21<00:57,  1.28s/trial, best loss: -1912.1498544145943]\r 54%|█████▍    | 52/96 [01:22<00:54,  1.23s/trial, best loss: -1912.1498544145943]\r 55%|█████▌    | 53/96 [01:23<00:51,  1.19s/trial, best loss: -1912.1498544145943]\r 56%|█████▋    | 54/96 [01:24<00:48,  1.15s/trial, best loss: -1912.1498544145943]\r 57%|█████▋    | 55/96 [01:28<01:17,  1.88s/trial, best loss: -1912.1498544145943]\r 58%|█████▊    | 56/96 [01:29<01:06,  1.66s/trial, best loss: -1912.1498544145943]\r 59%|█████▉    | 57/96 [01:30<00:58,  1.49s/trial, best loss: -1912.1498544145943]\r 61%|██████▏   | 59/96 [01:31<00:39,  1.07s/trial, best loss: -1912.1498544145943]\r 65%|██████▍   | 62/96 [01:34<00:35,  1.04s/trial, best loss: -1912.1498544145943]\r 66%|██████▌   | 63/96 [01:35<00:34,  1.04s/trial, best loss: -1912.1498544145943]\r 67%|██████▋   | 64/96 [01:37<00:40,  1.26s/trial, best loss: -1912.1498544145943]\r 69%|██████▉   | 66/96 [01:39<00:35,  1.17s/trial, best loss: -1912.1498544145943]\r 70%|██████▉   | 67/96 [01:40<00:33,  1.14s/trial, best loss: -1912.1498544145943]\r 72%|███████▏  | 69/96 [01:42<00:29,  1.10s/trial, best loss: -1912.1498544145943]\r 73%|███████▎  | 70/96 [01:44<00:33,  1.31s/trial, best loss: -1912.1498544145943]\r 74%|███████▍  | 71/96 [01:45<00:30,  1.24s/trial, best loss: -1912.1498544145943]\r 75%|███████▌  | 72/96 [01:48<00:34,  1.44s/trial, best loss: -1912.1498544145943]\r 76%|███████▌  | 73/96 [01:49<00:30,  1.33s/trial, best loss: -1912.1498544145943]\r 77%|███████▋  | 74/96 [01:50<00:27,  1.25s/trial, best loss: -1912.1498544145943]\r 78%|███████▊  | 75/96 [01:51<00:24,  1.19s/trial, best loss: -1912.1498544145943]\r 80%|████████  | 77/96 [01:52<00:16,  1.12trial/s, best loss: -1912.1498544145943]\r 81%|████████▏ | 78/96 [01:53<00:16,  1.08trial/s, best loss: -1912.1498544145943]\r 82%|████████▏ | 79/96 [01:54<00:16,  1.05trial/s, best loss: -1912.1498544145943]\r 83%|████████▎ | 80/96 [01:56<00:20,  1.25s/trial, best loss: -1912.1498544145943]\r 84%|████████▍ | 81/96 [01:57<00:17,  1.19s/trial, best loss: -1912.1498544145943]\r 85%|████████▌ | 82/96 [01:59<00:20,  1.44s/trial, best loss: -1912.1498544145943]\r 86%|████████▋ | 83/96 [02:00<00:17,  1.33s/trial, best loss: -1912.1498544145943]\r 88%|████████▊ | 84/96 [02:01<00:14,  1.24s/trial, best loss: -1912.1498544145943]\r 89%|████████▊ | 85/96 [02:02<00:13,  1.18s/trial, best loss: -1912.1498544145943]\r 91%|█████████ | 87/96 [02:04<00:09,  1.11s/trial, best loss: -1912.1498544145943]\r 93%|█████████▎| 89/96 [02:06<00:07,  1.07s/trial, best loss: -1912.1498544145943]\r 94%|█████████▍| 90/96 [02:08<00:07,  1.27s/trial, best loss: -1912.1498544145943]\r 95%|█████████▍| 91/96 [02:09<00:06,  1.21s/trial, best loss: -1912.1498544145943]\r 96%|█████████▌| 92/96 [02:10<00:04,  1.16s/trial, best loss: -1912.1498544145943]\r 97%|█████████▋| 93/96 [02:11<00:03,  1.11s/trial, best loss: -1912.1498544145943]\r 98%|█████████▊| 94/96 [02:12<00:02,  1.08s/trial, best loss: -1912.1498544145943]\r 99%|█████████▉| 95/96 [02:15<00:01,  1.63s/trial, best loss: -1912.1498544145943]\r100%|██████████| 96/96 [02:16<00:00,  1.45s/trial, best loss: -1912.1498544145943]\r100%|██████████| 96/96 [02:16<00:00,  1.42s/trial, best loss: -1912.1498544145943]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Trials: 96: 96 succeeded, 0 failed, 0 cancelled.\n"
     ]
    }
   ],
   "source": [
    " # Run fmin within an MLflow run context so that each hyperparameter configuration is logged as a child run of a parent\n",
    "# run called \"xgboost_models\" .\n",
    "with mlflow.start_run(run_name='xgboost_models'):\n",
    "  best_params = fmin(\n",
    "    fn=train_model, \n",
    "    space=search_space, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=96,\n",
    "    trials=spark_trials,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d60988e-6bbc-45e1-9ef2-1eb4a71d5d63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of Best Run: 1555.615725330372\n"
     ]
    }
   ],
   "source": [
    "best_run = mlflow.search_runs(order_by=['metrics.RMSE ASC']).iloc[0]\n",
    "print(f'RMSE of Best Run: {best_run[\"metrics.RMSE\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98863a8b-b447-4299-909f-8de61a7f5d0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: run_id                                                        9b4fb095dc994a1faafa91ac2aab1ea9\nexperiment_id                                                                  376818718792203\nstatus                                                                                FINISHED\nartifact_uri                                 dbfs:/databricks/mlflow-tracking/3768187187922...\nstart_time                                                    2023-06-15 10:50:50.232000+00:00\nend_time                                                      2023-06-15 10:50:58.999000+00:00\nmetrics.stopped_iteration                                                                245.0\nmetrics.RMSE                                                                       1555.615725\nmetrics.best_iteration                                                                   195.0\nmetrics.validation-rmse                                                            1554.834269\nparams.seed                                                                                123\nparams.boosting_type                                                                      gbdt\nparams.colsample_bytree                                                     0.8336223494478767\nparams.num_boost_round                                                                    1000\nparams.min_child_samples                                                                 135.0\nparams.metric                                                                             rmse\nparams.feature_pre_filter                                                                False\nparams.verbose_eval                                                                       True\nparams.min_data_in_leaf                                                                    142\nparams.reg_alpha                                                             2.361712031398684\nparams.early_stopping_rounds                                                                50\nparams.maximize                                                                           None\nparams.max_depth                                                                             2\nparams.learning_rate                                                        0.0310284166055511\nparams.custom_metric                                                                      None\nparams.num_leaves                                                                           34\nparams.reg_lambda                                                           0.9696321447579459\ntags.mlflow.user                                                        james.twose@cinqict.nl\ntags.mlflow.databricks.notebookRevisionID                                        1686826259389\ntags.mlflow.source.name                                       /Users/james.twose@cinqict.nl/ML\ntags.mlflow.databricks.notebookPath                           /Users/james.twose@cinqict.nl/ML\ntags.mlflow.runName                                                          vaunted-sheep-661\ntags.mlflow.source.type                                                               NOTEBOOK\ntags.mlflow.log-model.history                [{\"artifact_path\":\"model\",\"signature\":{\"inputs...\ntags.mlflow.databricks.notebookID                                              376818718792203\ntags.mlflow.databricks.webappURL                     https://westeurope-c2.azuredatabricks.net\ntags.mlflow.databricks.workspaceID                                                        None\ntags.mlflow.databricks.notebook.commandID                                                 None\ntags.mlflow.databricks.workspaceURL                                                       None\ntags.mlflow.databricks.cluster.info                                                       None\ntags.mlflow.databricks.cluster.id                                                         None\ntags.mlflow.databricks.cluster.libraries                                                  None\nName: 0, dtype: object"
     ]
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5875175c-bccb-4539-8528-3b80ef1bcf2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'disruption_prediction'.\n2023/06/15 10:55:35 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: disruption_prediction, version 1\nCreated version '1' of model 'disruption_prediction'.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"disruption_prediction\"\n",
    "new_model_version = mlflow.register_model(f\"runs:/{best_run.run_id}/model\", model_name)\n",
    " \n",
    "# Registering the model takes a few seconds, so add a small delay\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c848c51-0a3a-4412-9989-8591f87bf4d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: <ModelVersion: aliases=[], creation_timestamp=1686826535248, current_stage='Production', description='', last_updated_timestamp=1686826676807, name='disruption_prediction', run_id='9b4fb095dc994a1faafa91ac2aab1ea9', run_link='', source='dbfs:/databricks/mlflow-tracking/376818718792203/9b4fb095dc994a1faafa91ac2aab1ea9/artifacts/model', status='READY', status_message='', tags={}, user_id='5109889680170896', version='1'>"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "# Promote the new model version to Production\n",
    "client.transition_model_version_stage(\n",
    "  name=model_name,\n",
    "  version=new_model_version.version,\n",
    "  stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c02063-f336-4f2b-a5b5-4fbf46b906a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/15 10:59:00 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - mlflow (current: 2.4.1, required: mlflow==2.4)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1626.2048003495559\n"
     ]
    }
   ],
   "source": [
    "# Try the model that is saved in the production models\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/production\")\n",
    " \n",
    "# Sanity-check: This should match the RMSE logged by MLflow\n",
    "print(f'RMSE: {np.sqrt(mean_squared_error(y_test, model.predict(X_test)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e824d10b-1542-4801-9d52-40c2dd431c95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
